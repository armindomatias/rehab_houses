{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data to be between 0 and 1 (255 is the max value of a pixel)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 60k datapoints/images\n",
    "- Each image/datapoint is a matrix of 28 by 28 pixels \n",
    "- This is modeled as a list that have another 28 lists inside, representing the rows and then each of these rows/lists have another 28 items inside representing all the columns \n",
    "\n",
    "- Remember that a Neural Network will receive an image to then classify it, but a NN does not receive a 28x28 matrix, it receives a flattened matrix, that means all pixels in one column (like a big base vector with many axis)\n",
    "\n",
    "- So we need to flattened the whole thing to then train the model and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13762db50>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clearly is a 5 and then y_train have the output of the classification which is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLATTENING THE IMAGES FOR INPUT IN NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)\n",
    "X_test_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reshape function takes a certain matrix and transform it to any possible matrix you want \n",
    "- In this case we transform the matrix that holds all the images, kept the same amount of images, the same amount of items inside the matrix (list), but change each of the elements/items inside this matrix (the images)\n",
    "- We took each image/element of the matrix, which is another matrix of 28x28 and we flattened this to have only one column instead of 28 and put everything that was inside each of the rows of each column into a flattened one (each element have a list with 28 elements/pixels, so we took out this on each one)\n",
    "- That means, then if we take 28 stuff out of 28 stuffs, thats a flattened array of 28*28 stuffs = 784 elements for the NN to work on \n",
    "\n",
    "- The NN will takes this 784 elements and classify the image based on that (output layer), but there could be as well hidden layers that will work as filters to help the output layers decide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the Layers of our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armindomatias/.pyenv/versions/rehab_houses/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we are modeling our NN, defining the layers\n",
    "\n",
    "- So, here we say that the NN will have 10 possible output layers (numbers from 0 to 9)\n",
    "\n",
    "- Also that the input of the NN will be 784x1 flattened matrix -> a vector, not a matrix -> NN expects vectors not matrixs\n",
    "\n",
    "- We are not defining Hidden Layers (stps to filter from input layers to output layers)\n",
    "\n",
    "- And by saying a Dense Layer, we are saying that all Layers in the NN are connected to each other, that means, all of the 784 layers are connected to all of the 10 output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile -> Is just add necessary Hyperparameters for the model \n",
    "\n",
    "- OPTIMIZER: choose an optimizer from some available to optimize the minimization of the loss function \n",
    "- LOSS: choose the loss function you want to minimize with the optimizer -> depends on the case -> look into the loss functions available to choose \n",
    "- METRICS: is the metric you want to maximize the model for, in this case we choose accuracy bc is the thing we are saying the model is what we value the most, the accuracy of the model predicting right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641us/step - accuracy: 0.8136 - loss: 0.7171\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638us/step - accuracy: 0.9130 - loss: 0.3086\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642us/step - accuracy: 0.9210 - loss: 0.2821\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639us/step - accuracy: 0.9239 - loss: 0.2751\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - accuracy: 0.9281 - loss: 0.2610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13f785010>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_flattened, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model with 5 epochs -> that means the model will run all the data 5 times to iterate -> that means each time will run it in a feedback loop learning about the previous and adjusting for the next one \n",
    "- Too many epochs could lead to overfitting, too few could lead to underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - accuracy: 0.9162 - loss: 0.3003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2669002115726471, 0.925599992275238]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy on the test set of 93% \n",
    "- Around 9 out 10 images are predicted correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f795c10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33729370e-02, 3.83097216e-07, 7.25023299e-02, 9.53435242e-01,\n",
       "       3.72712105e-03, 1.36335209e-01, 1.93996902e-06, 9.99815702e-01,\n",
       "       1.01218164e-01, 6.88167870e-01], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the max of all the predictions, so the highest probability of the prediction\n",
    "np.argmax(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the highest probability of the prediction\n",
    "y_pred_labels = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(7), np.int64(2), np.int64(1), np.int64(0), np.int64(4)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 in 5 were correct here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[ 962,    0,    1,    1,    0,    5,    6,    4,    1,    0],\n",
       "       [   0, 1115,    2,    2,    0,    1,    4,    2,    9,    0],\n",
       "       [   7,   10,  920,   15,   13,    4,   13,   11,   36,    3],\n",
       "       [   4,    0,   19,  908,    1,   34,    3,   11,   23,    7],\n",
       "       [   1,    1,    2,    2,  928,    0,   12,    4,    9,   23],\n",
       "       [  10,    3,    2,   24,   12,  783,   18,    6,   28,    6],\n",
       "       [  12,    3,    4,    1,    7,   10,  918,    2,    1,    0],\n",
       "       [   1,    5,   23,    6,    7,    0,    0,  958,    4,   24],\n",
       "       [   8,    7,    6,   20,    9,   25,   10,   12,  872,    5],\n",
       "       [  11,    7,    1,   10,   40,    7,    0,   33,    8,  892]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Matrix of the predictions and the actual values\n",
    "    # The diagonal is the correct predictions\n",
    "    # The rest is the incorrect predictions\n",
    "tf.math.confusion_matrix(labels=y_test, predictions=y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To create a Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden layer will be something between the Input and the output \n",
    "- So we can create it with: 'keras.layers.Dense()', here we use Dense or another\n",
    "- And then we put inside input of that, which will be the 784 pixels (base vector), since the hidden layer is between the input and the output and we can choose the quantity of hidden layers, for example a 100, for the activation there are many. \n",
    "- Then we just add an output layer to finish the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armindomatias/.pyenv/versions/rehab_houses/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8646 - loss: 0.4663\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9615 - loss: 0.1291\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9736 - loss: 0.0886\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9823 - loss: 0.0621\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9857 - loss: 0.0466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13f7a14c0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(784,), activation='relu'), # Hidden Layer\n",
    "    keras.layers.Dense(10, activation='sigmoid') # Output Layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train_flattened, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9723 - loss: 0.0931\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[ 973,    0,    1,    0,    0,    0,    2,    1,    3,    0],\n",
       "       [   0, 1126,    2,    1,    0,    1,    2,    0,    3,    0],\n",
       "       [   6,    0, 1007,    2,    4,    0,    3,    3,    6,    1],\n",
       "       [   2,    0,    8,  983,    0,    5,    0,    5,    1,    6],\n",
       "       [   2,    0,    2,    0,  951,    1,    5,    1,    0,   20],\n",
       "       [   3,    1,    0,    5,    2,  869,    5,    1,    3,    3],\n",
       "       [   5,    3,    1,    1,    1,    4,  943,    0,    0,    0],\n",
       "       [   2,    3,    9,    5,    2,    0,    0,  991,    1,   15],\n",
       "       [  12,    0,    3,    9,    3,    5,    5,    4,  929,    4],\n",
       "       [   3,    3,    0,    2,    5,    4,    0,    3,    1,  988]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_flattened, y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_flattened)\n",
    "y_pred_labels = [np.argmax(i) for i in y_pred]\n",
    "\n",
    "tf.math.confusion_matrix(labels=y_test, predictions=y_pred_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Much better model with 97% accuracy by adding a new hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehab_houses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
